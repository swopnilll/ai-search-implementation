# Ai-search-implementation
Building an Intelligent Search and Q&A System for Task Management
=================================================================

1\. Introduction: Build a Smarter Search for Our Task Management App
--------------------------------------------------------------------

**Overall Goal:** This document outlines the requirements for developing an intelligent search and question-answering (Q&A) system for a Jira-like task management application. The primary objective is to enhance how users interact with project and ticket information, moving beyond simple keyword matching to a system that understands the underlying meaning and context of user queries. Essentially, the goal is to equip the application with a capability to comprehend textual data semantically.

**The Problem We're Solving:** Current search functionalities often rely on exact keyword matches. This means users might struggle to find relevant information if they don't use the precise terminology stored within project descriptions, ticket details, or status updates. For instance, a search for "resolve login issue" might miss a relevant ticket titled "fix authentication bug." The aim is to overcome this limitation by implementing a system capable of understanding user intent and the semantic relationships within the application's data.

**Key Technologies Introduction:** To achieve this, the project leverages several advanced AI and database technologies:

-   **Vector Embeddings:** This technique transforms text (like project descriptions or ticket details) into numerical representations, often called vectors. These numbers capture the semantic meaning, allowing the system to understand that concepts like "bug fix" and "issue resolution" are related because their numerical representations will be close in a high-dimensional space.
-   **Retrieval-Augmented Generation (RAG):** This is an AI framework designed to improve the accuracy and reliability of answers generated by large language models (LLMs). It works by first retrieving relevant pieces of information from a specific knowledge base (in this case, the application's data stored as embeddings) and then providing this information to the LLM as context to generate a well-founded answer. This process is analogous to giving the AI an "open-book exam," ensuring answers are based on actual data rather than just the model's general knowledge.
-   **Hybrid Search:** This approach combines the strengths of semantic search (understanding meaning via vectors) and traditional search methods like keyword matching or SQL filtering (finding exact terms or filtering by structured data like project IDs or statuses). This synergy allows for more comprehensive and precise search results that cater to queries involving both conceptual understanding and specific criteria.

**Your Role:** This document serves as a technical specification for applicants undertaking this development task. It aims to provide a clear and comprehensive understanding of the required components, functionalities, technical concepts, and deliverables. The subsequent sections detail the specific tasks involved in building the embedding pipeline, the RAG-based Q&A system, and the hybrid search functionality.

2\. Part 1: Teaching the AI About Your Data - The Embedding Pipeline
--------------------------------------------------------------------

**The Big Picture:** Before the application can intelligently search or answer questions about its content, it must first process and understand the existing data -- project names, descriptions, ticket details, status updates, and potentially other relevant text. This foundational step involves converting this textual information into a format the AI can work with: vector embeddings. This entire process constitutes the embedding pipeline.

**Concept: What are Vector Embeddings? (The AI's Understanding)**

-   **Simple Explanation:** Vector embeddings are numerical representations -- essentially, long lists of numbers (vectors) -- that capture the semantic meaning or essence of text. They function by mapping words, sentences, or even paragraphs to points in a multi-dimensional mathematical space. In this space, pieces of text with similar meanings are located closer to each other. For example, the vector representing "fix user interface bug" would be mathematically nearer to the vector for "resolve visual glitch in UI" than it would be to "implement new database schema."
-   **Analogy:** Embeddings can be thought of as unique "digital fingerprints" or even "DNA for data". Just as a fingerprint uniquely identifies a person, a vector embedding uniquely represents the meaning of a piece of text in a structured, numerical format that computers can effectively process and compare.
-   **Why Numbers?** Computers excel at mathematical operations but struggle with the abstract nuances of human language directly. Embeddings bridge this gap by translating the complex relationships and meanings inherent in language into a numerical format. This allows the system to perform mathematical calculations, such as measuring the "distance" or "similarity" between different pieces of text based on their vector representations. The famous example illustrating this capability is the vector relationship: vector("King") - vector("Man") + vector("Woman") results in a vector very close to vector("Queen"), demonstrating how these numerical representations capture underlying semantic relationships.

**Tool: Why OpenAI's `text-embedding-3-small`?**

-   **Explanation:** The chosen model for generating these embeddings is OpenAI's `text-embedding-3-small`. This model is selected because it offers a strong balance between several key factors: embedding quality (its ability to accurately capture semantic meaning), performance (speed of generating embeddings), and cost-effectiveness. It is an improvement over previous models like `text-embedding-ada-002` and is specifically designed to be efficient. This model generates vectors with 1536 dimensions, providing a sufficiently rich representation for understanding the nuances in project and ticket data. The maximum input length it supports is 8191 tokens, although the chunking strategy (discussed next) will use much smaller segments.
-   **Strategic Choice:** Opting for `text-embedding-3-small` reflects a pragmatic approach suitable for a Jira-like application environment. Such applications can accumulate vast amounts of textual data over time. Therefore, an embedding model needs to be efficient and cost-effective, particularly if embeddings need to be generated or updated frequently (e.g., as new tickets are created or descriptions change). While larger models like `text-embedding-3-large` might capture even finer semantic details , `text-embedding-3-small` is deemed sufficient for grasping the core meaning of task management data, representing a practical trade-off between capability, speed, and cost for this specific use case.

**Process: Chunking - Breaking Down Information**

-   **Why Chunk?** AI models, including embedding models, have limitations on the amount of text they can process in one go, often referred to as the "context window". Feeding very long documents (e.g., a lengthy project history or a detailed ticket description with many comments) into an embedding model as a single unit can dilute the semantic representation; the resulting vector might become too general. Breaking the text into smaller, more focused "chunks" before embedding ensures that each vector represents a more specific piece of information. This leads to better results during retrieval, as queries are more likely to match specific, relevant chunks. It's akin to summarizing a book chapter by chapter, which is more informative than trying to capture the entire book's meaning in a single sentence. Smaller chunks also ensure that the input size stays well within the model's limits.
-   **How to Chunk:** The requirement is to split project and task descriptions (and potentially other relevant text fields) into segments ranging from 200 to 500 *tokens*. A token can be roughly thought of as a word or part of a word. This size range aims to capture meaningful context within each chunk without being excessively long. Crucially, no single chunk should exceed the embedding model's maximum token limit.
-   **Overlap Strategy:** Simply splitting text at fixed character or token counts can awkwardly sever sentences or ideas, losing important context at the boundaries. To mitigate this, an overlap strategy is recommended. This means that the end of one chunk should also form the beginning of the next chunk (e.g., overlapping by 50-100 tokens). This ensures that concepts or sentences spanning across potential split points are fully captured in at least one of the chunks, preserving semantic continuity.
-   **Example:** Consider a ticket description: "The user login process is failing intermittently. When users enter correct credentials, sometimes they receive a '500 Internal Server Error' instead of being redirected to their dashboard. This seems to happen more frequently during peak hours. We need to investigate the authentication service logs and check the load balancer configuration for potential bottlenecks. The issue was first reported in ticket #123 but seems more widespread now." This could be chunked (simplified example, not token-based):
    -   *Chunk 1:* "The user login process is failing intermittently. When users enter correct credentials, sometimes they receive a '500 Internal Server Error' instead of being redirected to their dashboard. This seems to happen more frequently during peak hours. We need to investigate the authentication service logs..." (Overlap starts here)
    -   *Chunk 2:* "...authentication service logs and check the load balancer configuration for potential bottlenecks. The issue was first reported in ticket #123 but seems more widespread now."

**Storage: Using Supabase with pgvector (The AI's Library)**

-   **Explanation:** Supabase, an open-source backend-as-a-service platform built on PostgreSQL, will serve as the database. Within Supabase, the `pgvector` extension will be utilized. `pgvector` is a powerful PostgreSQL extension specifically designed to handle vector data types; it allows storing these numerical embeddings directly within the relational database and provides functions for efficient similarity searching.
-   **Metadata:** Storing the vector embedding alone is insufficient. It's crucial to store associated metadata alongside each vector. For this project, the minimum required metadata includes the `project_id` and `ticket_id` corresponding to the original text chunk. This linkage is essential for tracing search results back to the specific items within the task management application and presenting relevant source information to the user. The original text chunk itself should also be stored for context retrieval.
-   **Indexing:** As the number of embeddings grows, performing a sequential scan to find similar vectors becomes computationally expensive and slow. To ensure fast retrieval, database indexes must be created on the embedding column. `pgvector` supports specialized index types like IVFFlat and HNSW (Hierarchical Navigable Small World), which significantly accelerate similarity searches by organizing the vectors in a way that allows the database to quickly narrow down the search space. Using an appropriate index (e.g., `vector_cosine_ops` for cosine similarity searches) is critical for achieving acceptable query performance.
-   **Platform Choice:** The decision to use Supabase combined with `pgvector` offers a significant advantage: it integrates vector storage and search capabilities directly within the primary relational database. This avoids the complexity of setting up, managing, and synchronizing data between a traditional relational database (for project/ticket metadata) and a separate, dedicated vector database. This integrated approach simplifies the overall system architecture, potentially reduces operational overhead, and allows leveraging existing PostgreSQL expertise.

**Action: API Task - Generating Embeddings (`POST /api/embeddings/generate`)**

-   **Purpose:** This API endpoint serves as the trigger mechanism for the embedding pipeline. When called with specific project or ticket identifiers, it orchestrates the process of fetching the relevant text, chunking it, generating embeddings via the OpenAI API, and storing the results in Supabase.
-   **Request Format:**
    -   `Content-Type: application/json`
    -   `Body:` Must be a JSON object containing the identifiers for the data to be embedded.

        JSON

        ```
        {
          "project_id": 123,
          "ticket_id": 456
        }

        ```

        (This example indicates that the text content associated with ticket 456 within project 123 should be processed.)
-   **Response Format:**
    -   On successful completion, the API should return a JSON object confirming success and providing the unique identifiers (UUIDs or similar) assigned to each generated embedding chunk stored in the database.

        JSON

        ```
        {
          "status": "success",
          "embedding_ids": ["a1b2c3d4-e5f6-7890-abcd-ef1234567890", "b2c3d4e5-f6g7-8901-bcde-f1234567890a"]
        }

        ```

-   **Definition of Done (Specific to this task):**
    -   The endpoint correctly identifies and retrieves the source text based on the provided `project_id` and `ticket_id`.
    -   Text is chunked according to the specified size (200-500 tokens) and overlap requirements.
    -   The OpenAI API (`text-embedding-3-small`) is successfully called for each text chunk to generate a 1536-dimension vector embedding.
    -   Embeddings are normalized before storage (OpenAI models often provide normalized embeddings, but this should be confirmed/ensured).
    -   Each vector, along with its associated metadata (`project_id`, `ticket_id`, original chunk text), is successfully inserted into the designated Supabase table equipped with `pgvector`.
    -   The endpoint returns a `200 OK` (or similar success status) response containing the status and a list of the unique identifiers for the newly stored embeddings.

**Data: What Information Needs Embedding?**

-   **Priorities Explained:** Not all text data within the application contributes equally to semantic search and Q&A. Prioritization helps focus resources on the most valuable information first.

    -   **High Priority:**
        -   *Project Name & Description:* These fields contain the high-level goals, context, and core subject matter of projects.
        -   *Ticket Name & Description:* These hold the specifics of tasks, issues, features, including detailed problem reports, requirements, and discussion points. They are primary targets for user queries.
        -   *Status Changes (Potentially):* While lower priority than descriptions, comments associated with status changes (e.g., "Closed - resolved via workaround X") can sometimes contain valuable contextual keywords or explanations relevant to a ticket's history. Embedding these might be considered, but primarily the textual descriptions are key.
    -   **Low Priority:**
        -   *User Roles:* Generally less relevant for understanding the *content* of a project or ticket, unless roles are explicitly mentioned within descriptions in a meaningful way (e.g., "Task assigned to QA lead for testing"). Often better handled via SQL filters.
        -   *Timestamps:* Dates and times typically don't carry strong semantic meaning relevant for similarity search in this context, although they are crucial for filtering or sorting via traditional database queries. Embedding timestamps directly is usually not beneficial.

### Data to Vectorize - Priorities Table

| Data Field            | Priority | Rationale for Priority                                                                                   |
|-----------------------|----------|-----------------------------------------------------------------------------------------------------------|
| Project Name          | High     | Core identifier and subject of the project.                                                              |
| Project Description   | High     | Contains detailed context, goals, and scope; rich in semantic meaning.                                   |
| Ticket Name (Title)   | High     | Concise summary of the task/issue; often contains key terms.                                             |
| Ticket Description    | High     | Detailed explanation, steps to reproduce, requirements, discussion; primary source of semantic content.  |
| Status Change Notes   | Medium   | May contain contextual keywords or resolution details, but often less semantically dense than descriptions. |
| User Roles            | Low      | Primarily structural data; usually less relevant for semantic search unless embedded in descriptive text. |
| Timestamps            | Low      | Primarily for filtering/ordering; low semantic value for similarity matching in this context.             |


3\. Part 2: Answering User Questions - The RAG System
-----------------------------------------------------

**The Big Picture:** With the application's data processed into meaningful vector embeddings (Part 1), the next step is to build a system that can leverage this understanding to answer user questions directly. This is achieved using the Retrieval-Augmented Generation (RAG) pattern.

**Concept: What is Retrieval-Augmented Generation (RAG)?**

-   **Simple Explanation:** RAG is a technique that enhances the ability of Large Language Models (LLMs) to answer questions accurately by grounding their responses in specific, relevant information. It operates in two main phases: first, it *retrieves* pertinent data snippets from a designated knowledge source (our Supabase vector store containing project/ticket embeddings). Second, it *augments* the user's original question with this retrieved data, feeding both to an LLM to *generate* a contextually informed answer.
-   **Analogy:** RAG can be visualized as an AI performing research before answering a question. Instead of relying solely on its pre-trained, general knowledge (which might be outdated or lack specifics about *our* projects), the AI acts like a librarian who first consults relevant books and articles (retrieves data chunks) before synthesizing the information into a comprehensive answer or summary. Alternatively, it's like giving the AI an open-book test where it must base its answers on the provided text (the retrieved chunks).
-   **Benefits:** Implementing RAG offers several advantages:
    -   **Improved Accuracy:** Answers are based on actual data from the application, making them more factually accurate and relevant to the specific context of projects and tickets.
    -   **Reduced Hallucinations:** By providing specific context, RAG significantly reduces the likelihood of the LLM inventing plausible but incorrect information ("hallucinating").
    -   **Up-to-Date Information:** The system can answer questions based on the latest information embedded in the database without needing constant retraining of the underlying LLM. As new tickets are added and embedded, the RAG system can immediately utilize them.
    -   **Transparency:** RAG systems can often cite the sources (e.g., specific ticket IDs) used to generate an answer, increasing user trust and allowing for verification.

**Process: How RAG Answers Questions (Step-by-Step)**

1.  **User Asks:** The process begins when a user submits a question in natural language, for example: "What is the status of the UI redesign for Project Atlas?"
2.  **Embed Question:** The system takes the user's question and converts it into a vector embedding. Critically, this uses the *exact same* embedding model (`text-embedding-3-small`) that was used to embed the application's data. This ensures that the question vector and the data vectors exist in the same semantic space, allowing for meaningful similarity comparisons.
3.  **Vector Search:** The question embedding is used to query the Supabase vector store (`pgvector`). The database searches for the stored text chunk embeddings that are most mathematically similar to the question embedding. This is typically done using cosine similarity. The goal is to retrieve the top K (e.g., top 5) chunks that exceed a predefined similarity threshold (e.g., similarity score > 0.7).
4.  **Filter Results (MMR):** The raw list of top K similar chunks might contain redundant information. Maximal Marginal Relevance (MMR) filtering is applied to this list to select a subset of chunks that are both highly relevant to the question and diverse in the information they provide.
5.  **Augment Prompt:** The system constructs a new prompt for a generative LLM (e.g., OpenAI's GPT-3.5 or GPT-4). This prompt typically includes the original user question and the filtered, relevant text chunks retrieved in the previous steps, along with instructions for the LLM to answer the question based *only* on the provided context.
6.  **Generate Answer:** The augmented prompt is sent to the LLM. The LLM processes the request and generates a textual answer based on the information contained within the provided context chunks.
7.  **Return Answer:** The final, AI-generated answer is returned to the user. Optionally, the response can include references to the source documents (e.g., `ticket_id`s) from which the context was retrieved.

**Search: Finding Relevant Chunks (Vector Search)**

-   **Explanation:** Vector search operates by navigating the high-dimensional "meaning map" created by the embeddings. When a question is embedded, the system searches for the data points (chunk embeddings) that are the "nearest neighbors" to the question point in this space. The primary metric used to measure this proximity or similarity is *cosine similarity*. This measures the cosine of the angle between two vectors; a value closer to 1 indicates the vectors point in very similar directions, implying similar semantic meaning, while a value closer to 0 indicates less similarity.
-   **Threshold:** The requirement `similarity > 0.7` acts as a relevance filter. It ensures that only chunks whose meaning is significantly close to the question's meaning are considered potential context. Chunks below this threshold are discarded as likely irrelevant.
-   **Top K:** Retrieving the `top 5` most similar chunks (above the threshold) aims to provide the LLM with a sufficient amount of relevant context to formulate a comprehensive answer, without overwhelming it with too much information.

**Refinement: Getting Diverse Results with MMR (Avoiding Repetition)**

-   **Problem:** A simple top-K vector search might return multiple chunks that are highly similar not only to the query but also to each other. For instance, if several tickets discuss the same specific bug in slightly different ways, they might all rank highly. Providing such repetitive context to the LLM is inefficient and may not lead to the best possible answer.
-   **Solution (MMR):** Maximal Marginal Relevance (MMR) is employed to address this issue. MMR is a re-ranking algorithm that optimizes the selected set of documents (or chunks, in this case) for both relevance to the original query *and* diversity among the selected items.
-   **How it Works (Simplified):** MMR works iteratively. It starts by selecting the single chunk most similar to the query. Then, in subsequent steps, it evaluates remaining candidate chunks based on a combined score: how relevant the chunk is to the query, minus a penalty based on how similar that chunk is to the chunks *already selected*. A parameter (often denoted as lambda, λ) controls the trade-off: a higher lambda prioritizes relevance, while a lower lambda prioritizes diversity. By balancing these two factors, MMR aims to build a set of context chunks that cover different relevant aspects of the query without excessive overlap.
-   **Value Proposition:** The inclusion of MMR demonstrates a nuanced understanding of RAG pipeline optimization. Pure similarity search retrieves documents relevant to the query, but MMR ensures the *set* of retrieved documents provides broader, less redundant context. This is crucial for enabling the LLM to generate more comprehensive and informative answers, as it receives a richer, more varied set of information points related to the query, rather than potentially receiving the same point reiterated multiple times. This directly impacts the quality of the final generated answer.

**Action: API Task - Answering Queries (`POST /api/query`)**

-   **Purpose:** This endpoint is the user-facing interface for the RAG system. It accepts a question posed in natural language and returns a synthetically generated answer based on the information retrieved from the application's data.
-   **Request Format:**
    -   `Content-Type: application/json`
    -   `Body:` A JSON object containing the user's question.

        JSON

        ```
        {
          "question": "What's the main objective of the 'Phoenix' project?"
        }

        ```

-   **Response Format:**
    -   The API should return a JSON object containing the generated answer and information about the source documents used as context.

        JSON

        ```
        {
          "answer": "The main objective of Project Phoenix is to refactor the legacy authentication module to improve security and performance.",
          "sources": [
            { "ticket_id": 5, "similarity": 0.82 },
            { "ticket_id": 12, "similarity": 0.75 }
          ]
        }

        ```

        (The `similarity` score here likely refers to the initial cosine similarity score between the chunk and the query before MMR re-ranking).

**Definition of Done (Specific to this task):**

-   The endpoint successfully receives and parses the JSON request containing the question.
-   The question is embedded using the `text-embedding-3-small` model.
-   A vector search is performed in Supabase using cosine similarity, retrieving the top K (e.g., 5) chunks meeting the similarity threshold (e.g., > 0.7).
-   MMR filtering is applied to the retrieved set of chunks to select a diverse yet relevant subset.
-   A prompt is constructed that includes the original question and the filtered context chunks.
-   An appropriate OpenAI generative model (e.g., GPT-3.5-turbo, GPT-4) is called with the constructed prompt.
-   The LLM's response is parsed to extract the final answer.
-   The endpoint returns a `200 OK` response containing the answer and source information (e.g., ticket IDs and similarity scores) in the specified JSON format.

4\. Part 3: Advanced Search - Combining Keyword and Semantic Understanding (Hybrid Search)
------------------------------------------------------------------------------------------

**The Big Picture:** While the RAG system excels at answering natural language questions, users often need a more conventional search experience that blends the power of semantic understanding with the precision of keyword matching and structured data filtering. Hybrid search addresses this need by combining multiple retrieval techniques.

**Concept: Why Hybrid Search? (The Best of Both Worlds)**

-   **Limitations of Each:**

    -   *Vector Search (Semantic):* Excellent at understanding conceptual similarity and user intent, even with different wording. However, it can sometimes struggle with exact keyword matches, specific codes, identifiers, or jargon that might not have strong semantic representations in the general model. For example, searching for "Ticket TX-400" might not be well handled by semantic search alone if "TX-400" isn't semantically meaningful.
    -   *Keyword/SQL Search (Lexical/Relational):* Highly precise for finding exact terms, phrases, or filtering based on structured data fields (like status, project ID, assignee). However, it completely lacks semantic understanding; it cannot find conceptually related items if the exact keywords aren't present.
-   **Synergy:** Hybrid search leverages the strengths of both approaches simultaneously. It runs both a semantic (vector) search and a keyword/SQL-based search in parallel. The results from these disparate searches are then intelligently merged into a single, unified ranked list. This approach provides more robust and relevant results, particularly for complex queries that combine conceptual elements with specific terms or filters (e.g., finding "documentation tasks" related to "API authentication" within "Project Apollo" that are currently "In Progress").

-   **Keyword vs. Vector vs. Hybrid Search Comparison Table:**

| Feature         | Keyword Search (e.g., SQL LIKE, FTS/BM25) | Vector Search (Semantic)                                  | Hybrid Search (Combined)                                                  |
|----------------|--------------------------------------------|------------------------------------------------------------|-----------------------------------------------------------------------------|
| Underlying Tech| String matching, Inverted indexes          | Vector embeddings, Similarity metrics (cosine)             | Both keyword/FTS and vector search + Fusion Algorithm (RRF)               |
| Strengths      | Exact term matching, Filters, Speed        | Understands meaning, intent, synonyms                      | Best of both: semantic understanding + keyword precision                  |
| Weaknesses     | No semantic understanding, Sensitive to typos | Can miss specific keywords/codes, Less precise             | More complex to implement, Requires result merging                        |
| Best For       | Known items, Specific codes/IDs, Filtering | Conceptual search, Q&A, Discovery                          | Complex queries, Comprehensive results, Balancing relevance               |
| Example Query  | `description LIKE '%TX-400%'`              | `"tasks related to user login problems"`                   | `"UI bugs in Project X status: To Do"`                                    |

**Data Prep: Gathering Context with SQL (Filtering First)**

-   **Purpose:** For hybrid search queries that include specific filters (like a `project_id` or a `status`), it's efficient to first use standard SQL to narrow down the set of potentially relevant documents *before* performing more computationally intensive operations like keyword or vector search. This pre-filtering step significantly reduces the scope of data that needs to be processed further.

-   **Example SQL Query:** The following SQL query demonstrates this pre-filtering concept. It joins the `projects`, `tickets`, and `status` tables to retrieve textual information (`ticket_name`, `ticket_description`) but restricts the results based on optional `project_id` and `status` name parameters provided by the user.

    SQL

    ```
    SELECT
      p.project_id,
      p.name AS project_name,
      t.ticket_id,
      t.name AS ticket_name,
      t.description AS ticket_description,
      s.name AS status
    FROM projects p
    JOIN tickets t ON p.project_id = t.project_id
    JOIN status s ON t.status_id = s.status_id
    WHERE p.project_id = :project_id  -- Optional filter placeholder
      AND s.name = :status_filter;    -- Optional filter placeholder

    ```

    The output of such a query (a list of relevant tickets and their associated text) forms the candidate set for the subsequent keyword and vector search steps within the hybrid search process.
**Process: The Hybrid Search Steps**

1.  **Initial Filtering (SQL):** Check if the incoming hybrid search request contains filters like `project_id` or `status_filter`. If present, execute a SQL query (similar to the example above) to retrieve only the subset of tickets/projects matching these criteria. If no filters are provided, the scope remains the entire dataset (or relevant portion).
2.  **Keyword Search:** Within the (potentially pre-filtered) data scope, perform a keyword search. This could involve using SQL `LIKE` clauses for simple substring matching or leveraging a more sophisticated full-text search (FTS) engine if available (e.g., PostgreSQL's built-in FTS, potentially using BM25 ranking). Search relevant text fields (like `ticket_name`, `ticket_description`) for exact terms present in the user's query (e.g., "UX", "bug"). Generate a ranked list of results based on keyword relevance.
3.  **Vector Search:** Embed the user's full query ("Find UX bugs in Project X") using `text-embedding-3-small`. Perform a vector similarity search (using cosine similarity) against the embeddings corresponding to the documents within the (potentially pre-filtered) data scope. Retrieve the top K (e.g., 5) most semantically similar chunks/documents that exceed the similarity threshold (e.g., > 0.7). Generate a ranked list based on similarity scores.
4.  **Combine Results (RRF):** Take the two (or more) ranked lists generated by the keyword search and the vector search. Use the Reciprocal Rank Fusion (RRF) algorithm to merge these lists into a single, final ranked list of results.

**Combining: Merging Results with RRF (Fair Ranking)**

-   **Problem:** The scores produced by keyword search algorithms (like BM25 relevance scores from FTS) and vector search algorithms (like cosine similarity scores) operate on entirely different scales and distributions. Directly comparing, adding, or averaging these scores is mathematically unsound and often leads to skewed or unreliable final rankings, where one search method might unfairly dominate the results.
-   **Solution (RRF):** Reciprocal Rank Fusion (RRF) provides an elegant solution to this merging problem. RRF is a rank-based fusion method, meaning it primarily considers the *position* (rank) of a document within each individual result list, rather than its raw score.
-   **How it Works (Simplified):** For each document that appears in one or more of the input ranked lists, RRF calculates a score based on the reciprocal of its rank in each list. The formula is typically `Score(doc) = Σ (1 / (k + rank(doc in list_i)))` for each list `i` where the document appears. The constant `k` (commonly set to 60) helps dampen the influence of very low-ranked items. The scores from each list are summed up for each unique document. A key benefit is that documents ranking highly in *multiple* input lists (i.e., found by both keyword and vector search) naturally receive a higher combined RRF score and are thus boosted in the final merged ranking. This process produces a single, unified ranking that fairly integrates results from diverse search methods without requiring complex score normalization or tuning.
-   **Method Selection:** The adoption of RRF signifies a mature approach to hybrid search implementation. It acknowledges the inherent challenges of merging scores from heterogeneous retrieval systems and employs a standard, effective, and relatively parameter-light algorithm designed specifically for this purpose. It avoids the pitfalls of naive normalization techniques and leverages the principle of consensus between different search paradigms to improve the robustness and relevance of the final ranking.

**Action: API Task - Hybrid Search (`POST /api/hybrid-search`)**

-   **Purpose:** This endpoint provides the combined power of keyword/SQL filtering and semantic vector search. It accepts a user query along with optional filters and returns a single, ranked list of relevant items fused using RRF.

-   **Request Format:**

    -   `Content-Type: application/json`
    -   `Body:` A JSON object containing the query and optional filters.

        JSON

        ```
        {
          "query": "Find UX bugs in Project X",
          "project_id": 1,
          "status_filter": "To Do"
        }

        ```

        (Here, `project_id` and `status_filter` are optional parameters.)
-   **Response Format:**

    -   The API should return a JSON object containing a list of ranked results. Each result should indicate its source (keyword or vector) and potentially its final RRF score.

        JSON

        ```
        {
          "results": [
            {
              "ticket_id": 5,
              "text": "Fix navigation UI element overlap on mobile view...",
              "similarity": 0.82, // Present if source is 'vector'
              "source": "vector",
              "rrf_score": 0.0325
            },
            {
              "ticket_id": 25,
              "text": "Address UX feedback regarding button placement...",
              "source": "keyword",
              "rrf_score": 0.0169
            },
            //... other results
          ]
        }

        ```

-   **Definition of Done (Specific to this task):**

    -   The endpoint correctly parses the incoming query and any optional `project_id` or `status_filter` parameters.
    -   If filters are provided, an initial SQL query is executed to narrow the data scope.
    -   A keyword search is performed on the relevant text fields within the filtered scope.
    -   The user query is embedded, and a vector search is performed against embeddings within the filtered scope.
    -   Results from both the keyword and vector searches are collected.
    -   The RRF algorithm is implemented correctly to merge and re-rank these results into a single list.
    -   The endpoint returns a `200 OK` response containing the top N combined results, formatted as specified, including `ticket_id`, relevant text snippet, source (`keyword` or `vector`), and potentially the RRF score.
### Summary of API Endpoints

| Endpoint Path            | HTTP Method | Purpose                                                             | Key Input(s)                         | Key Output(s)                         |
|--------------------------|-------------|---------------------------------------------------------------------|--------------------------------------|----------------------------------------|
| `/api/embeddings/generate` | POST        | Trigger embedding generation for specific project/ticket data.      | `project_id`, `ticket_id`            | `status`, `embedding_ids`              |
| `/api/query`             | POST        | Answer a natural language question using RAG.                       | `question`                           | `answer`, `sources` (with similarity)  |
| `/api/hybrid-search`     | POST        | Perform hybrid search combining keyword/filter and semantic search. | `query`, `project_id?`, `status_filter?` | `results` (with source, RRF score)     |

5\. Part 4: Measuring Success and What to Deliver
-------------------------------------------------

**Evaluation: How We'll Know It Works (Success Criteria)**

Evaluating the success of this system involves assessing its correctness, performance, robustness, and the quality of its results.

-   **Correctness & Relevance:** The fundamental measure is whether the system provides useful and accurate information.
    -   Does the `/api/query` endpoint return answers that directly address the user's question and are factually supported by the retrieved context chunks?
    -   Does the `/api/hybrid-search` endpoint return items (tickets, projects) that are genuinely relevant to both the specific keywords and the semantic meaning of the query?
    -   Are the specified similarity thresholds (e.g., cosine similarity > 0.7 for vector search) correctly implemented and enforced?
-   **Performance:** The system must be responsive enough for a good user experience.
    -   Are the average response times for the `/api/query` and `/api/hybrid-search` endpoints below the target threshold of 1.5 seconds under typical load?
    -   Are database queries optimized? Specifically, confirm that appropriate vector indexes (e.g., HNSW or IVFFlat using `vector_cosine_ops` for cosine similarity) are created and utilized in Supabase to accelerate vector searches. Standard SQL queries should also be efficient.
-   **Error Handling:** The system should handle unexpected situations gracefully.
    -   Are potential failures during communication with external services (OpenAI API, Supabase database) properly caught and logged for debugging?
    -   Does the API return standard and informative HTTP error codes (e.g., 400 Bad Request for malformed input, 5xx for server-side issues) to the client?  
-   **Diversity (MMR):** The quality of RAG depends on diverse context.
    -   Does the context retrieval mechanism for `/api/query` effectively utilize MMR to avoid selecting highly redundant information chunks, thereby providing a more diverse set of sources for answer generation?  

**Deliverables: Your Checklist (Definition of Done - Overall Project)**

The successful completion of this task requires the delivery of functional code, documentation, and evidence of meeting the core requirements.

-   **Embedding Pipeline:**
    -   ✅ Text chunking implementation correctly adheres to size (200-500 tokens) and overlap specifications.
    -   ✅ Embeddings are successfully generated using OpenAI's `text-embedding-3-small` model.
    -   ✅ Generated vectors and required metadata (`project_id`, `ticket_id`, original chunk text) are accurately stored in the Supabase `pgvector` table.
    -   ✅ The `POST /api/embeddings/generate` endpoint is fully functional as per its specification.
-   **Question-Answering (RAG):**
    -   ✅ The `POST /api/query` endpoint functions correctly, receiving questions and returning AI-generated answers based on retrieved context.
    -   ✅ Vector search component uses cosine similarity and respects the defined relevance threshold (> 0.7).
    -   ✅ Maximal Marginal Relevance (MMR) is implemented and applied to the retrieved chunks before passing them to the LLM for generation.
    -   ✅ (Recommended) Basic logging is implemented to track queries, retrieved sources, and generated answers, potentially flagging queries with very low similarity scores or ambiguous results.
-   **Hybrid Search:**
    -   ✅ The `POST /api/hybrid-search` endpoint is fully functional as per its specification.
    -   ✅ SQL-based filtering using optional `project_id` and `status_filter` parameters works correctly.
    -   ✅ Both keyword search and vector search components are implemented and functional within the hybrid workflow.
    -   ✅ Reciprocal Rank Fusion (RRF) is correctly implemented to merge and re-rank results from the different search methods.
-   **Code & Documentation:**
    -   ✅ Clean, well-structured, and reasonably commented code base submitted via a Git repository link.
    -   ✅ API documentation (e.g., a comprehensive README.md file or preferably an OpenAPI/Swagger specification) detailing endpoint usage, request/response formats, and setup instructions.
    -   ✅ Brief notes summarizing performance testing methodology and observed average response times for `/api/query` and `/api/hybrid-search` under sample load conditions.
-   **Database Setup:**
    -   ✅ Clear definition of the Supabase table schema used for storing embeddings and metadata.
    -   ✅ Evidence that appropriate vector indexes (e.g., IVFFlat or HNSW with `vector_cosine_ops`) are created on the embedding column in the Supabase table to ensure efficient querying.

**Going Further: Bonus Points Explained**

Applicants demonstrating capabilities beyond the core requirements will be viewed favorably.

-   **Real-time Updates:** This involves implementing mechanisms (e.g., database triggers, message queue listeners) to automatically invoke the `/api/embeddings/generate` endpoint whenever a new ticket is created or an existing ticket's relevant fields (name, description) are updated. This ensures the AI's knowledge base remains continuously synchronized with the application data without manual intervention.
-   **Caching:** This involves implementing a caching layer (e.g., using Redis or in-memory cache) to store the results of frequently executed or identical queries (for both `/api/query` and `/api/hybrid-search`). If the same query is received again within a short timeframe, the cached result can be returned instantly, improving perceived performance and reducing the computational load and costs associated with repeated embedding lookups, LLM calls, and database searches.

6\. Conclusion: Your Skills and How to Submit
---------------------------------------------

**Skills You'll Demonstrate:** Successfully completing this task will demonstrate proficiency in several key areas relevant to modern AI application development:

-   Working with vector databases and extensions (specifically Supabase with `pgvector`).
-   Implementing core AI patterns like Retrieval-Augmented Generation (RAG) and Hybrid Search.
-   Utilizing external AI APIs effectively (OpenAI for embeddings and generation).
-   Designing, building, and documenting robust RESTful APIs.
-   Understanding and implementing text processing pipelines, including chunking strategies.
-   Applying advanced ranking and filtering techniques (MMR, RRF).

**Submission Instructions:** To submit your work for evaluation, please provide the following:

1.  **Code Repository:** A link to a Git repository (e.g., GitHub, GitLab) containing the complete source code for the implemented solution. Ensure the repository includes instructions for setup and execution.
2.  **API Documentation:** Clear documentation for the implemented API endpoints (`/api/embeddings/generate`, `/api/query`, `/api/hybrid-search`). An OpenAPI (Swagger) specification is preferred, but a detailed README.md file covering endpoints, request/response formats, and usage examples is acceptable.
3.  **Performance Metrics:** A brief document or section in the README outlining the approach taken for performance testing and reporting the observed average response times for the `/api/query` and `/api/hybrid-search` endpoints under representative conditions.
